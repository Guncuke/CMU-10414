{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b38216e2"
   },
   "source": [
    "# Homework 3: Building an NDArray library\n",
    "\n",
    "In this homework, you will build a simple backing library for the processing that underlies most deep learning systems: the n-dimensional array (a.k.a. the NDArray).  Up until now, you have largely been using numpy for this purpose, but this homework will walk you through developing what amounts to your own (albeit much more limited) variant of numpy, which will support both CPU and GPU backends.  What's more, unlike numpy (and even variants like PyTorch), you won't simply call out to existing highly-optimized variants of matrix multiplication or other manipulation code, but actually write your own versions that are reasonably competitive will the highly optimized code backing these standard libraries (by some measure, i.e., \"only 2-3x slower\" ... which is a whole lot better than naive code that can easily be 100x slower).  This class will ultimately be integrated into `needle`, but for this assignment you can _only_ focus on the ndarray module, as this will be the only subject of the tests.\n",
    "\n",
    "**Note**: To avoid exhausting limited GPU resources in Colab, start by using CPU runtime for coding and testing non-GPU functions. Switch to GPU runtime when testing CUDA or GPU-accelerated code. This approach ensures efficient GPU usage and prevents running out of resources during critical tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "db5a484c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
      "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-i1mhon8y\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-i1mhon8y\n",
      "  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pybind11 in /home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages (2.13.6)\n"
     ]
    }
   ],
   "source": [
    "# Code to set up the assignment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "# !mkdir -p 10714\n",
    "# %cd /content/drive/MyDrive/10714\n",
    "# !git clone https://github.com/dlsys10714/hw3.git\n",
    "# %cd /content/drive/MyDrive/10714/hw3\n",
    "\n",
    "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "09eedbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pybind11/include (found version \"2.13.6\")\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Jan 20 11:56:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 45%   32C    P8             17W /  450W |     864MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:41:00.0 Off |                  Off |\n",
      "| 47%   33C    P8             16W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        Off |   00000000:81:00.0 Off |                  Off |\n",
      "| 47%   33C    P8             14W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        Off |   00000000:A1:00.0 Off |                  Off |\n",
      "| 45%   32C    P8             25W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2033495      C   python                                        420MiB |\n",
      "|    0   N/A  N/A   2036215      C   python                                        420MiB |\n",
      "|    0   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.9 8.9 8.9 8.9\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/user002/code/AI_fram/CMU-10414/hw3/build\n",
      "make[1]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[2]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[3]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[3]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-310-x86_64-linux-gnu.so\u001b[0m\n",
      "make[3]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[3]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[1]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n"
     ]
    }
   ],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae72a5",
   "metadata": {},
   "source": [
    "The make command reads the Makefile in the current directory. The Makefile contains rules that define how to build targets (like executables or libraries). For each target specified in the Makefile, make checks the timestamps of the target file and its dependencies (like .c, .cpp, or .h files). If any dependency has been modified recently, it must rebuild the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "glK6Q7Ik-2cJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=./python\n",
      "env: NEEDLE_BACKEND=nd\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "e7aadcf9"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "899bc262"
   },
   "source": [
    "## Getting familiar with the NDArray class\n",
    "\n",
    "As you get started with this homework, you should first familiarize yourself with the `NDArray.py` class we have provided as a starting point for the assignment.  The code is fairly brief (it's ~500 lines, but a lot of these are comments provided for the functions you'll implement).\n",
    "\n",
    "At its core, the NDArray class is a Python wrapper for handling operations on generic n-dimensional arrays.  Recall that virtually any such array will be stored internally as a vector of floating point values, i.e.,\n",
    "\n",
    "```c++\n",
    "float data[size];\n",
    "```\n",
    "\n",
    "and then the actual access to different dimensions of the array are all handled by additional fields (such as the array shape, strides, etc) that indicates how this \"flat\" array maps to n-dimensional structure.  In order to achieve any sort of reasonable speed, the \"raw\" operations (like adding, binary operations, but also more structured operations like matrix multiplication, etc), all need to be written at some level in some native language like C++ (including e.g., making CUDA calls).  But a large number of operations likes transposing, broadcasting, sub-setting of matrices, and other, can all be handled by just adjusting the high-level structure of the array, like it's strides.\n",
    "\n",
    "The philosophy behind the NDArray class is that we want _all_ the logic for handling this structure of the array to be written in Python.  Only the \"true\" low level code that actually performs the raw underlying operations on the flat vector (as well as the code to manage these flat vectors, as they might need to e.g., be allocated on GPUs), is written in C++.  The precise nature of this separation will likely start to make more sense to you as you work through the assignment, but generally speaking everything that can be done in Python, is done in Python; often e.g., at the cost of some inefficiencies ... we call `.compact()` (which copies memory) liberally in order to make the underlying C++ implementations simpler.\n",
    "\n",
    "In more detail, there are five fields within the NDArray class that you'll need to be familiar with (note that the real class member these all these fields is preceded by an underscore, e.g., `_handle`, `_strides`, etc, some of which are then exposed as a public property ... for all your code it's fine to use the internal, underscored version).\n",
    "\n",
    "1. `device` - A object of type `BackendDevice`, which is a simple wrapper that contains a link to the underlying device backend (e.g., CPU or CUDA).\n",
    "2. `handle` - A class objected that stores the underlying memory of the array.  This is allocated as a class of type `device.Array()`, though this allocation all happens in the provided code (specifically the `NDArray.make` function), and you don't need to worry about calling it yourself.\n",
    "3. `shape` - A tuple specifying the size of each dimension in the array.\n",
    "4. `strides` - A tuple specifying the strides of each dimension in the array.\n",
    "5. `offset` - An integer indicating where in the underlying `device.Array` memory the array actually starts (it's convenient to store this so we can more easily manage pointing back to existing memory, without having to track allocations).\n",
    "\n",
    "By manipulating these fields, even pure Python code can perform a lot of the needed operations on the array, such as permuting the dimensions (i.e., transposing), broadcasting, and more.  And then for the raw array manipulation calls, the `device` class has a number of methods (implemented in C++) that contains the necessary implementations.\n",
    "\n",
    "There are a few points to note:\n",
    "\n",
    "* Internally, the class can use _any_ efficient means of operating on arrays of data as a \"device\" backend.  Even, for example, a numpy array, but where instead of actually using the `numpy.ndarray` to represent the n-dimensional array, we just represent a \"flat\" 1D array in numpy, then call the relevant numpy methods to implement all the needed operators on this raw memory.  This is precisely what we do in the `ndarray_backend_numpy.py` file, which essentially provided a \"stub reference\" that just uses numpy for everything.  You can use this class to help you  better debug your own \"real\" implementations for the \"native\" CPU and GPU backends.\n",
    "* Of particular importance for many of your Python implementations will be the `NDArray.make` call:\n",
    "```python\n",
    "def make(shape, strides=None, device=None, handle=None, offset=0):\n",
    "```\n",
    "which creates a new NDArray with the given shape, strides, device, handle, and offset.  If `handle` is not specified (i.e., no pre-existing memory is referenced), then the call will allocate the needed memory, but if handle _is_ specified then no new memory is allocated, but the new NDArray points the same memory as the old one.  It is important to efficient implementations that as many of your functions as possible _don't_ allocate new memory, so you will want to use this call in many cases to accomplish this.\n",
    "* The NDArray has a `.numpy()` call that converts the array to numpy.  This is _not_ the same as the \"numpy_device\" backend: this creates an actual `numpy.ndarray` that is equivalent to the given NDArray, i.e., the same dimensions, shape, etc, though not necessarily the same strides (Pybind11 will reallocate memory for matrices that are returned in this manner, which can change the striding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc7c24db"
   },
   "source": [
    "## Part 1: Python array operations\n",
    "\n",
    "As a starting point for your class, implement the following functions in the `ndarray.py` file:\n",
    "\n",
    "- `reshape()`\n",
    "- `permute()`\n",
    "- `broadcast_to()`\n",
    "- `__getitem__()`\n",
    "\n",
    "The inputs/outputs of these functions are all described in the docstring of the function stub.  It's important to emphasize that _none_ of these functions should reallocate memory, but should instead return NDArrays that share the same memory with `self`, and just use clever manipulation of shape/strides/etc in order to obtain the necessary transformations.\n",
    "\n",
    "To get started you can refer to the hints mentioned in the class slides for transpose, broadcast and slicing operator.\n",
    "\n",
    "One thing to note is that the `__getitem__()` call, unlike numpy, will never change the number of dimensions in the array.  So e.g., for a 2D NDArray `A`, `A[2,2]` would return a still-2D with one row and one column.  And e.g. `A[:4,2]` would return a 2D NDarray with 4 rows and 1 column.\n",
    "\n",
    "You can rely on the `ndarray_backend_numpy.py` module for all the code in this section.  You can also look at the results of equivalent numpy operations (the test cases should illustrate what these are).\n",
    "\n",
    "After implementing these functions, you should pass/submit the following tests.  Note that we test all of these four functions within the test below, and you can incrementally try to pass more tests as you implement each additional function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "835c3880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 126 deselected / 10 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_permute[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 10%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_permute[cpu-params1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 20%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_permute[cpu-params2] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 30%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reshape[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 40%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reshape[cpu-params1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 50%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_getitem[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_getitem[cpu-params1] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 70%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_getitem[cpu-params2] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 80%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_getitem[cpu-params3] \u001b[32mPASSED\u001b[0m\u001b[32m              [ 90%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_broadcast_to[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m10 passed\u001b[0m, \u001b[33m126 deselected\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"(permute or reshape or broadcast or getitem) and cpu and not compact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "071b3063"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_python_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21f48d27"
   },
   "source": [
    "## Part 2: CPU Backend - Compact and setitem\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cpu.cc`:\n",
    "* `Compact()`\n",
    "* `EwiseSetitem()`\n",
    "* `ScalarSetitem()`\n",
    "\n",
    "To see why these are all in the same category, let's consider the implementation of the `Compact()` function.  Recall that a matrix is considered compact if it is layed out sequentially in memory in \"row-major\" form (but really a generalization of row-many to higher dimensional arrays), i.e. with the last dimension first, followed by the second to last dimension, etc, all the way to the first.  In our implementation, we also require that the total size of allocated backend array be equal to the size of the array (i.e., the underlying array also can't have any data before or after the array data, which e.g., implies that the `.offset` field equals zero).\n",
    "\n",
    "Now let's consider, using a 3D array as a an example, of how a compact call might work.  Here `shape` and `strides` are the shape and strides of the matrix being compacted (i.e., before we have compacted it).\n",
    "\n",
    "```c++\n",
    "cnt = 0;\n",
    "for (size_t i = 0; i < shape[0]; i++)\n",
    "    for (size_t j = 0; j < shape[1]; j++)\n",
    "        for (size_t k = 0; k < shape[2]; k++)\n",
    "            out[cnt++] = in[strides[0]*i + strides[1]*j + strides[2]*k];\n",
    "```\n",
    "In other words, we're converting from a stride-based representation of the matrix to a purely sequential one.\n",
    "\n",
    "Now, the challenge in implementing `Compact()` is that you want the method to work for any number of input dimensions.  It's easy to specialize for different fixed-dimension-size arrays, but for a generic implementation, you'll want to think about how to do this same operation where you effectively want a \"variable number of for loops\".  As a hint, one way to do this is to maintain a vector of indices (of size equal to the number of dimensions), and then manually increment them in a loop (including a \"carry\" operation when any of the reaches their maximum size).\n",
    "\n",
    "However, if you get really stuck with this, you can alway use the fact that we're probably not going to ask you to deal with matrices of more than 6 dimensions (though we _will_ use 6 dimensions, for the im2col operation we discussed in class).\n",
    "\n",
    "\n",
    "#### The connection to setitem\n",
    "The setitem functionality, while seemingly quite different, is actually intimately related to `Compact()`.  `__setitem()__` is the Python function called when setting some elements of the object, i.e.,\n",
    "```python\n",
    "A[::2,4:5,9] = 0 # or = some_other_array\n",
    "```\n",
    "How would you go about implementing this?  In the `__getitem()__` call above, you already implemented a method to take a subset of a matrix without copying (but just modifying strides).  But how would you actually go about _setting_ elements of this array?  In virtually all the other settings in this homework, we call `.compact()` before setting items in an output array, but in this case it doesn't work: calling `.compact()` would copy the subset array to some new memory, but the whole point of the `__setitem__()` call is that we want to modify existing memory.\n",
    "\n",
    "If you think about this for a while, you'll realize that the answer looks a lot like `.compact()` but in reverse.  If we want to assign a (itself already compact) right hand side matrix to a `__getitem()__` results, then we need to here like `shape` and `stride` be those fields of the _output_ matrix.  Then we could implement the setitem call as follows\n",
    "\n",
    "```c++\n",
    "cnt = 0;\n",
    "for (size_t i = 0; i < shape[0]; i++)\n",
    "    for (size_t j = 0; j < shape[1]; j++)\n",
    "        for (size_t k = 0; k < shape[2]; k++)\n",
    "            out[strides[0]*i + strides[1]*j + strides[2]*k] = in[cnt++]; // or \"= val;\"\n",
    "```\n",
    "Due to this similarity, if you implement your indexing strategy in a modular fashion, you'll be able to reuse it between the `Compact()` call and the `EwiseSetitem()` and `ScalarSetitem()` calls.\n",
    "\n",
    "**Note**: Don't forget to run make before executing the tests to rebuild your modified C++ code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "758969a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 121 deselected / 15 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-transpose] \u001b[32mPASSED\u001b[0m\u001b[32m            [  6%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-broadcast_to] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 13%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-reshape1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 20%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-reshape2] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 26%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-reshape3] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 33%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-getitem1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 40%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-getitem2] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 46%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cpu-transposegetitem] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 53%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cpu-params1] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cpu-params2] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 73%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cpu-params0] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 80%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cpu-params1] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 86%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cpu-params2] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 93%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cpu-params3] \u001b[32mPASSED\u001b[0m\u001b[32m       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m15 passed\u001b[0m, \u001b[33m121 deselected\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"(compact or setitem) and cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74c98f6c"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cpu_compact_setitem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9df3e8fb"
   },
   "source": [
    "## Part 3: CPU Backend - Elementwise and scalar operations\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cpu.cc`:\n",
    "\n",
    "* `EwiseMul()`, `ScalarMul()`\n",
    "* `EwiseDiv()`, `ScalarDiv()`\n",
    "* `ScalarPower()`\n",
    "* `EwiseMaximum()`, `ScalarMaximum()`\n",
    "* `EwiseEq()`, `ScalarEq()`\n",
    "* `EwiseGe()`, `ScalarGe()`\n",
    "* `EwiseLog()`\n",
    "* `EwiseExp()`\n",
    "* `EwiseTanh()`\n",
    "\n",
    "You can look at the included\n",
    "`EwiseAdd()` and `ScalarAdd()` functions (plus the invocations from `NDArray` in order to understand the required format of these functions.\n",
    "\n",
    "Note that unlike the remaining functions mentioned here, we do not include function stubs for each of these functions.  This is because, while you can implement these naively just through implementing each function separately, though this will end up with a lot of duplicated code.  You're welcome to use e.g., C++ templates or macros to address this problem (but these would only be exposed internally, not to the external interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "22943d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 113 deselected / 23 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-multiply] \u001b[32mPASSED\u001b[0m\u001b[32m     [  4%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [  8%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-add] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 13%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 17%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-equal] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 21%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape0-greater_than] \u001b[32mPASSED\u001b[0m\u001b[32m [ 26%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-multiply] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 30%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m       [ 34%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-add] \u001b[32mPASSED\u001b[0m\u001b[32m          [ 39%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 43%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-equal] \u001b[32mPASSED\u001b[0m\u001b[32m        [ 47%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cpu-shape1-greater_than] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_max[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 56%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_max[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[32m             [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_mul[cpu] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 65%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_div[cpu] \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 69%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_power[cpu] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 73%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_maximum[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m               [ 78%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_eq[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m                    [ 82%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_ge[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m                    [ 86%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_log[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m                    [ 91%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_exp[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m                    [ 95%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_tanh[cpu] \u001b[32mPASSED\u001b[0m\u001b[33m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_power[cpu]\n",
      "  /home/user002/code/AI_fram/CMU-10414/hw3/tests/hw3/test_ndarray.py:390: RuntimeWarning: invalid value encountered in power\n",
      "    np.power(A, 0.5), (B**0.5).numpy(), atol=1e-5, rtol=1e-5\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================ \u001b[32m23 passed\u001b[0m, \u001b[33m\u001b[1m113 deselected\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 0.28s\u001b[0m\u001b[33m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"(ewise_fn or ewise_max or log or exp or tanh or (scalar and not setitem)) and cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fddba8d9"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cpu_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7be9fe7c"
   },
   "source": [
    "## Part 4: CPU Backend - Reductions\n",
    "\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cpu.cc`:\n",
    "\n",
    "* `ReduceMax()`\n",
    "* `ReduceSum()`\n",
    "\n",
    "In general, the reduction functions `.max()` and `.sum()` in NDArray take the max or sum across a specified axis specified by the `axis` argument (or across the entire array when `axis=None`); note that we don't support axis being a set of axes, though this wouldn't be too hard to add if you desired (but it's not in the interface you should implement for the homework).\n",
    "\n",
    "Because summing over individual axes can be a bit tricky, even for compact arrays, these functions (in Python) in Python simplify things by permuting the last axis to the be the one reduced over (this is what the `reduce_view_out()` function in NDArray does), then compacting the array.  So for your `ReduceMax()` and `ReduceSum()` functions you implement in C++, you can assume that both the input and output arrays are contiguous in memory, and you want to just reduce over contiguous elements of size `reduce_size` as passed to the C++ functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "de5382b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 128 deselected / 8 selected                              \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params0-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 12%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params1-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 25%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params2-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 37%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params3-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 50%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params0-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 62%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params1-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 75%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params2-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [ 87%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params3-cpu] \u001b[32mPASSED\u001b[0m\u001b[32m           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m====================== \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m128 deselected\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"reduce and cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1f1f484"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cpu_reductions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35e1e9c0"
   },
   "source": [
    "## Part 5: CPU Backend - Matrix multiplication\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cpu.cc`:\n",
    "\n",
    "* `Matmul()`\n",
    "* `MatmulTiled()`\n",
    "* `AlignedDot()`\n",
    "\n",
    "The first implementation, `Matmul()` can use the naive three-nested-for-loops algorithm for matrix multiplication.  However, the `MatmulTiled()` performs the same matrix multiplication on memory laid out in tiled form, i.e., as a contiguous 4D array\n",
    "```c++\n",
    "float[M/TILE][N/TILE][TILE][TILE];\n",
    "```\n",
    "\n",
    "Make sure to initialize the output matrix to zero before populating it with the correct values during matrix multiplication.\n",
    "\n",
    "Note that the Python `__matmul__` code already does the conversion to tiled form when all sizes of the matrix multiplication are divisible by `TILE`, so your code just needs to implement the multiplication in this form.  In order to make the methods efficient, you will want to make use of (after you implement it), the `AlignedDot()` function, which will enable the compiler to efficiently make use of vector operations and proper caching.  The output matrix will also be in the tiled form above, and the Python backend will take care of the conversion to a normal 2D array.\n",
    "\n",
    "Note that in order to get the most speedup possible from you tiled version, you may want to use the clang compiler with colab instead of gcc.  To do this, run the following command before building your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "28771449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 126 deselected / 10 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_matmul[16-16-16-cpu] Fatal Python error: Aborted\n",
      "\n",
      "Current thread 0x000070e223d2a440 (most recent call first):\n",
      "  File \"/home/user002/code/AI_fram/CMU-10414/hw3/python/needle/backend_ndarray/ndarray.py\", line 539 in __matmul__\n",
      "  File \"/home/user002/code/AI_fram/CMU-10414/hw3/tests/hw3/test_ndarray.py\", line 365 in test_matmul\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/python.py\", line 159 in pytest_pyfunc_call\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_callers.py\", line 103 in _multicall\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_manager.py\", line 120 in _hookexec\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_hooks.py\", line 513 in __call__\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/python.py\", line 1627 in runtest\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 174 in pytest_runtest_call\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_callers.py\", line 103 in _multicall\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_manager.py\", line 120 in _hookexec\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_hooks.py\", line 513 in __call__\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 242 in <lambda>\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 341 in from_call\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 241 in call_and_report\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 132 in runtestprotocol\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/runner.py\", line 113 in pytest_runtest_protocol\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_callers.py\", line 103 in _multicall\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_manager.py\", line 120 in _hookexec\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_hooks.py\", line 513 in __call__\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/main.py\", line 362 in pytest_runtestloop\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_callers.py\", line 103 in _multicall\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_manager.py\", line 120 in _hookexec\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_hooks.py\", line 513 in __call__\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/main.py\", line 337 in _main\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/main.py\", line 283 in wrap_session\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/main.py\", line 330 in pytest_cmdline_main\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_callers.py\", line 103 in _multicall\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_manager.py\", line 120 in _hookexec\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pluggy/_hooks.py\", line 513 in __call__\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/config/__init__.py\", line 175 in main\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/_pytest/config/__init__.py\", line 201 in console_main\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pytest/__main__.py\", line 9 in <module>\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/runpy.py\", line 86 in _run_code\n",
      "  File \"/home/user002/miniconda3/envs/LLMs/lib/python3.10/runpy.py\", line 196 in _run_module_as_main\n",
      "\n",
      "Extension modules: mkl._mklinit, mkl._py_mkl_service, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, _brotli (total: 16)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"matmul and cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "5e0a7443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 9 items / 8 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ submit_ndarray_cpu_matmul ___________________________\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_ndarray_cpu_matmul>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "func_name = 'ndarray_cpu_matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mstart_submission\u001b[39;49;00m(func_name):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\" Begin a submisssion to the mugrade server \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        server_url, protocol = get_server_url_protocol(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        response = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                                 params = {\u001b[33m\"\u001b[39;49;00m\u001b[33muser_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                                           \u001b[33m\"\u001b[39;49;00m\u001b[33mfunc_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: func_name},\u001b[90m\u001b[39;49;00m\n",
      "                                 verify=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m response.status_code != \u001b[94m200\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresponse.text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m response.json()[\u001b[33m\"\u001b[39;49;00m\u001b[33msubmission_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       KeyError: 'submission_key'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:103: KeyError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1msubmit_ndarray_cpu_matmul\u001b[0m - KeyError: 'submission_key'\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[31m in 1.03s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cpu_matmul\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "0d3e131e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Found pybind11: /home/user002/miniconda3/envs/LLMs/lib/python3.10/site-packages/pybind11/include (found version \"2.13.6\")\n",
      "-- Found cuda, building cuda backend\n",
      "Mon Jan 20 10:59:44 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 45%   32C    P8             17W /  450W |     864MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:41:00.0 Off |                  Off |\n",
      "| 47%   33C    P8             15W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 4090        Off |   00000000:81:00.0 Off |                  Off |\n",
      "| 47%   33C    P8             14W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 4090        Off |   00000000:A1:00.0 Off |                  Off |\n",
      "| 45%   32C    P8             25W /  450W |      15MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2033495      C   python                                        420MiB |\n",
      "|    0   N/A  N/A   2036215      C   python                                        420MiB |\n",
      "|    0   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   3273389      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "-- Autodetected CUDA architecture(s):  8.9 8.9 8.9 8.9\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/user002/code/AI_fram/CMU-10414/hw3/build\n",
      "make[1]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[2]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[3]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
      "make[3]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "[ 50%] Built target ndarray_backend_cpu\n",
      "make[3]: Entering directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[3]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "[100%] Built target ndarray_backend_cuda\n",
      "make[2]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n",
      "make[1]: Leaving directory '/home/user002/code/AI_fram/CMU-10414/hw3/build'\n"
     ]
    }
   ],
   "source": [
    "!export CXX=/usr/bin/clang++ && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c44486db"
   },
   "source": [
    "## Part 6: CUDA Backend - Compact and setitem\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cuda.cu`:\n",
    "* `Compact()`\n",
    "* `EwiseSetitem()`\n",
    "* `ScalarSetitem()`\n",
    "\n",
    "\n",
    "For this portion, you'll implement the compact and setitem calls in the CUDA backend.  This is fairly similar to the C++ version, however, depending on how you implemented that function, there could also be some substantial differences.  We specifically want to highlight a few differences between the C++ and the CUDA implementations, however.\n",
    "\n",
    "First, as with the example functions implemented in the CUDA backend code, for all the functions above you will actually want to implement two functions: the basic functions listed above that you will call from Python, and the corresponding CUDA kernels that will actually perform the computation.  For the most part, we only provide the prototype for the \"base\" function in the `ndarray_backend_cuda.cu` file, and you will need to define and implement the kernel function yourself.  However, to see how these work, for the `Compact()` call we are providing you with the _complete_ `Compact()` call, and the function prototype for the `CompactKernel()` call.\n",
    "\n",
    "One thing you may notice is the seemingly odd use of a `CudaVec` struct, which is a struct used to pass shape/stride parameters.  In the C++ version we used the STL `std::vector` variables to store these inputs (and the same is done in the base `Compact()` call, but CUDA kernels cannot operation on STL vectors, so something else is needed).  Furthermore, although we _could_ convert the vectors to normal CUDA arrays, this would be rather cumbersome, as we would need to call `cudaMalloc()`, pass the parameters as integer pointers, then free them after the calls.  Of course such memory management is needed for the actual underlying data in the array, but it seems like overkill to do it for just passing a variable-sized small tuple of shape/stride values.  The solution is to create a struct that has a \"maximize\" size for the number of dimensions an array can have, and then just store the actual shape/stride data in the first entries of these fields.  This is all done by the included `CudaVec` struct and `VecToCuda()` function, and you can just use these as provided for all the CUDA kernels that require passing shape/strides to the kernel itself.\n",
    "\n",
    "The other (more conceptual) big difference between the C++ and CUDA implementations of `Compact()` is that in C++ you will typically loop over the elements of the non-compact array sequentially, which allows you to perform some optimizations with respect to computing the corresponding indices between the compact and non-compact arrays.  In CUDA, you cannot do this, and will need to implement code that can directly map from an index in the compact array to one in the strided array.\n",
    "\n",
    "As before, we recommend you implement your code in such as way that it can easily be re-used between the `Compact()`, and `Setitem()` calls.  As a short note, remember that if you want to call a (separate, non-kernel) function from kernel code, you need to define it as a `__device__` function. In CUDA, `__global__` is used to define a function that runs on the GPU and is called from the CPU host, while `__device__` defines a function that runs and is called only on the GPU from other GPU function code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "480d127f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 121 deselected / 15 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-transpose] /home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [192,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [193,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [194,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [195,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [196,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [197,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [198,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [199,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [200,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [201,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [202,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [203,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [204,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [205,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [206,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [207,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [208,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [209,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [210,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [211,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [212,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [213,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [214,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [215,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [216,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [217,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [218,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [219,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [220,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [221,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [222,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [223,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [224,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [225,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [226,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [227,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [228,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [229,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [230,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [231,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [232,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [233,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [234,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [235,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [236,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [237,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [238,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [239,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [240,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [241,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [242,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [243,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [244,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [245,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [246,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [247,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [248,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [249,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [250,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [251,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [252,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [253,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [254,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [255,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [128,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [129,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [130,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [131,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [132,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [133,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [134,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [135,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [136,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [137,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [138,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [139,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [140,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [141,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [142,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [143,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [144,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [145,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [146,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [147,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [148,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [149,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [150,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [151,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [152,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [153,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [154,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [155,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [156,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [157,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [158,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [159,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [32,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [33,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [34,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [35,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [36,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [37,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [38,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [39,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [40,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [41,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [42,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [43,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [44,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [45,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [46,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [47,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [48,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [49,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [50,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [51,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [52,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [53,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [54,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [55,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [56,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [57,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [58,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [59,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [60,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [61,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [62,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [63,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [64,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [65,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [66,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [67,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [68,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [69,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [70,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [71,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [72,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [73,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [74,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [75,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [76,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [77,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [78,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [79,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [80,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [81,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [82,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [83,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [84,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [85,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [86,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [87,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [88,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [89,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [90,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [91,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [92,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [93,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [94,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [95,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [96,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [97,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [98,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [99,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [100,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [101,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [102,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [103,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [104,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [105,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [106,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [107,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [108,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [109,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [110,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [111,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [112,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [113,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [114,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [115,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [116,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [117,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [118,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [119,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [120,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [121,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [122,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [123,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [124,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [125,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [126,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [127,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [0,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [1,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [2,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [3,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [4,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [5,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [6,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [7,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [8,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [9,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [10,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [11,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [12,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [13,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [14,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [15,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [16,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [17,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [18,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [19,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [20,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [21,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [22,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [23,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [24,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [25,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [26,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [27,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [28,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [29,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [30,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [31,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [160,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [161,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [162,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [163,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [164,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [165,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [166,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [167,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [168,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [169,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [170,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [171,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [172,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [173,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [174,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [175,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [176,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [177,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [178,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [179,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [180,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [181,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [182,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [183,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [184,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [185,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [186,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [187,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [188,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [189,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [190,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "/home/user002/code/AI_fram/CMU-10414/hw3/src/ndarray_backend_cuda.cu:101: void needle::cuda::CompactKernel(const float *, float *, unsigned long, needle::cuda::CudaVec, needle::cuda::CudaVec, unsigned long): block: [0,0,0], thread: [191,0,0] Assertion `false && \"Not Implemented\"` failed.\n",
      "\u001b[31mFAILED\u001b[0m\u001b[31m           [  6%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-broadcast_to] \u001b[31mFAILED\u001b[0m\u001b[31m        [ 13%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-reshape1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 20%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-reshape2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 26%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-reshape3] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 33%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-getitem1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 40%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-getitem2] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 46%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_compact[cuda-transposegetitem] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 53%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cuda-params0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cuda-params1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 66%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_ewise[cuda-params2] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 73%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cuda-params0] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 80%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cuda-params1] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 86%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cuda-params2] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 93%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_setitem_scalar[cuda-params3] \u001b[31mFAILED\u001b[0m\u001b[31m      [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-transpose] _________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d02ef0>, 'np_fn': <function <lambda> at 0x7dc190d01d80>, 'shape': (4, 4)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        lhs = nd_fn(A).compact()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m lhs.is_compact(), \u001b[33m\"\u001b[39;49;00m\u001b[33marray is not compact\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        rhs = np_fn(_A)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(lhs.numpy(), rhs, atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:95: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = <[RuntimeError('device-side assert triggered') raised in repr()] NDArray object at 0x7dc19f93ea40>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mnumpy\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"convert to a numpy array\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.device.to_numpy(\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[96mself\u001b[39;49;00m._handle, \u001b[96mself\u001b[39;49;00m.shape, \u001b[96mself\u001b[39;49;00m.strides, \u001b[96mself\u001b[39;49;00m._offset\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:198: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________ test_compact[cuda-broadcast_to] ________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d03010>, 'np_fn': <function <lambda> at 0x7dc190d02f80>, 'shape': (4, 1, 4)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 1, 4), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-reshape1] __________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d03130>, 'np_fn': <function <lambda> at 0x7dc190d030a0>, 'shape': (4, 3)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 3), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-reshape2] __________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d03250>, 'np_fn': <function <lambda> at 0x7dc190d031c0>, 'shape': (16, 16)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (16, 16), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-reshape3] __________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d03370>, 'np_fn': <function <lambda> at 0x7dc190d032e0>, 'shape': (2, 4, 2, 2, 2, 2, ...)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (2, 4, 2, 2, 2, 2, ...), strides = None, device = cuda(), handle = None\n",
      "offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-getitem1] __________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d03490>, 'np_fn': <function <lambda> at 0x7dc190d03400>, 'shape': (8, 8)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (8, 8), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_________________________ test_compact[cuda-getitem2] __________________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d035b0>, 'np_fn': <function <lambda> at 0x7dc190d03520>, 'shape': (8, 8, 2, 2, 2, 2)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (8, 8, 2, 2, 2, 2), strides = None, device = cuda(), handle = None\n",
      "offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_____________________ test_compact[cuda-transposegetitem] ______________________\u001b[0m\n",
      "\n",
      "params = {'nd_fn': <function <lambda> at 0x7dc190d036d0>, 'np_fn': <function <lambda> at 0x7dc190d03640>, 'shape': (7, 8)}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose(),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: np.broadcast_to(X, shape=(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.broadcast_to((\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m4\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m4\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                    \u001b[94m2\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                ),  \u001b[90m# testing for compaction of large ndims array\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape(\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.reshape((\u001b[94m16\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m)),\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:], \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m4\u001b[39;49;00m:, \u001b[94m4\u001b[39;49;00m:]},\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m8\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m:\u001b[94m8\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m:\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: (\u001b[94m7\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m),\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.transpose()[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m X: X.permute((\u001b[94m1\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))[\u001b[94m3\u001b[39;49;00m:\u001b[94m7\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "        ids=[\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtranspose\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mbroadcast_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreshape3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mgetitem2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtransposegetitem\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_compact\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, np_fn, nd_fn = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnp_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33mnd_fn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randint(low=\u001b[94m0\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:89: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (7, 8), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________ test_setitem_ewise[cuda-params0] _______________________\u001b[0m\n",
      "\n",
      "params = {'lhs': ((4, 5, 6), (slice(1, 2, 1), slice(0, 1, 1), slice(0, 1, 1))), 'rhs': ((7, 7, 7), (slice(1, 2, 1), slice(0, 1, 1), slice(0, 1, 1)))}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m6\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[:\u001b[94m2\u001b[39;49;00m, :\u001b[94m3\u001b[39;49;00m, :\u001b[94m4\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_ewise\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        lhs_shape, lhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        rhs_shape, rhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*lhs_shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*rhs_shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:168: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________ test_setitem_ewise[cuda-params1] _______________________\u001b[0m\n",
      "\n",
      "params = {'lhs': ((4, 5, 6), (slice(1, 4, 2), slice(0, 1, 1), slice(0, 1, 1))), 'rhs': ((7, 7, 7), (slice(1, 3, 1), slice(0, 1, 1), slice(0, 1, 1)))}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m6\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[:\u001b[94m2\u001b[39;49;00m, :\u001b[94m3\u001b[39;49;00m, :\u001b[94m4\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_ewise\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        lhs_shape, lhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        rhs_shape, rhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*lhs_shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*rhs_shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:168: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m_______________________ test_setitem_ewise[cuda-params2] _______________________\u001b[0m\n",
      "\n",
      "params = {'lhs': ((4, 5, 6), (slice(1, 3, 1), slice(2, 5, 1), slice(2, 6, 1))), 'rhs': ((7, 7, 7), (slice(0, 2, 1), slice(0, 3, 1), slice(0, 4, 1)))}\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m:\u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "            {\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m6\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: ShapeAndSlices(\u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m, \u001b[94m7\u001b[39;49;00m)[:\u001b[94m2\u001b[39;49;00m, :\u001b[94m3\u001b[39;49;00m, :\u001b[94m4\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            },\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_ewise\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        lhs_shape, lhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mlhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        rhs_shape, rhs_slices = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mrhs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*lhs_shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*rhs_shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:168: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m______________________ test_setitem_scalar[cuda-params0] _______________________\u001b[0m\n",
      "\n",
      "params = ((4, 5, 6), (slice(1, 2, 1), slice(2, 3, 1), slice(3, 4, 1)))\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m::\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, ::\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_scalar\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, slices = params\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:193: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m______________________ test_setitem_scalar[cuda-params1] _______________________\u001b[0m\n",
      "\n",
      "params = ((4, 5, 6), (slice(1, 4, 1), slice(2, 3, 1), slice(3, 4, 1)))\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m::\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, ::\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_scalar\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, slices = params\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:193: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m______________________ test_setitem_scalar[cuda-params2] _______________________\u001b[0m\n",
      "\n",
      "params = ((4, 5, 6), (slice(0, 4, 1), slice(2, 5, 1), slice(3, 4, 1)))\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m::\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, ::\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_scalar\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, slices = params\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:193: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[31m\u001b[1m______________________ test_setitem_scalar[cuda-params3] _______________________\u001b[0m\n",
      "\n",
      "params = ((4, 5, 6), (slice(1, 4, 2), slice(2, 5, 1), slice(0, 6, 2)))\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "        [\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[:\u001b[94m4\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "            ShapeAndSlices(\u001b[94m4\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m)[\u001b[94m1\u001b[39;49;00m::\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m:\u001b[94m5\u001b[39;49;00m, ::\u001b[94m2\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "        ],\u001b[90m\u001b[39;49;00m\n",
      "    )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_setitem_scalar\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        shape, slices = params\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      ">       A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:193: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:597: in array\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n",
      "    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
      "\n",
      "    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n",
      "    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n",
      "    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n",
      "        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n",
      "        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n",
      "        array._offset = offset\u001b[90m\u001b[39;49;00m\n",
      "        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           RuntimeError: device-side assert triggered\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-transpose]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-broadcast_to]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-reshape1]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-reshape2]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-reshape3]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-getitem1]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-getitem2]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_compact[cuda-transposegetitem]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_ewise[cuda-params0]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_ewise[cuda-params1]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_ewise[cuda-params2]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_scalar[cuda-params0]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_scalar[cuda-params1]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_scalar[cuda-params2]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_setitem_scalar[cuda-params3]\u001b[0m - RuntimeError: device-side assert triggered\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m15 failed\u001b[0m, \u001b[33m121 deselected\u001b[0m\u001b[31m in 0.85s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"(compact or setitem) and cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "98a83634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 9 items / 8 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________ submit_ndarray_cuda_compact_setitem ______________________\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_ndarray_cuda_compact_setitem>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "func_name = 'ndarray_cuda_compact_setitem'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mstart_submission\u001b[39;49;00m(func_name):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\" Begin a submisssion to the mugrade server \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        server_url, protocol = get_server_url_protocol(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        response = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                                 params = {\u001b[33m\"\u001b[39;49;00m\u001b[33muser_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                                           \u001b[33m\"\u001b[39;49;00m\u001b[33mfunc_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: func_name},\u001b[90m\u001b[39;49;00m\n",
      "                                 verify=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m response.status_code != \u001b[94m200\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresponse.text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m response.json()[\u001b[33m\"\u001b[39;49;00m\u001b[33msubmission_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       KeyError: 'submission_key'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:103: KeyError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1msubmit_ndarray_cuda_compact_setitem\u001b[0m - KeyError: 'submission_key'\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[31m in 0.91s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cuda_compact_setitem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aa2a94a"
   },
   "source": [
    "## Part 7: CUDA Backend - Elementwise and scalar operations\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cuda.cu`:\n",
    "\n",
    "* `EwiseMul()`, `ScalarMul()`\n",
    "* `EwiseDiv()`, `ScalarDiv()`\n",
    "* `ScalarPower()`\n",
    "* `EwiseMaximum()`, `ScalarMaximum()`\n",
    "* `EwiseEq()`, `ScalarEq()`\n",
    "* `EwiseGe()`, `ScalarGe()`\n",
    "* `EwiseLog()`\n",
    "* `EwiseExp()`\n",
    "* `EwiseTanh()`\n",
    "\n",
    "Again, we don't provide these function prototypes, and you're welcome to use C++ templates or macros to make this implementation more compact.  You will also want to uncomment the appropriate regions of the Pybind11 code once you've implemented each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ee2d6dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 113 deselected / 23 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-multiply] \u001b[31mFAILED\u001b[0m\u001b[31m    [  4%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m      [  8%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-add] \u001b[32mPASSED\u001b[0m\u001b[31m         [ 13%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 17%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-equal] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 21%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape0-greater_than] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-multiply] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 30%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m      [ 34%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-add] \u001b[32mPASSED\u001b[0m\u001b[31m         [ 39%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 43%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-equal] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 47%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_fn[cuda-shape1-greater_than] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_max[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 56%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_max[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m            [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_mul[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 65%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_div[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 69%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_power[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 73%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_maximum[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 78%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_eq[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 82%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_scalar_ge[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 86%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_log[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 91%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_exp[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                   [ 95%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_ewise_tanh[cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                  [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape0-multiply] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0fd00>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:227: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mmultiply\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a * b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:442: in __mul__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_mul, \u001b[96mself\u001b[39;49;00m.device.scalar_mul\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_mul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_mul'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape0-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0fd90>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:228: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:449: in __truediv__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_div, \u001b[96mself\u001b[39;49;00m.device.scalar_div\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_div'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_div'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape0-subtract] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0feb0>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:230: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:435: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:453: in __neg__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m * (-\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:442: in __mul__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_mul, \u001b[96mself\u001b[39;49;00m.device.scalar_mul\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_mul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_mul'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_ewise_fn[cuda-shape0-equal] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0ff40>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:231: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mequal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a == b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:467: in __eq__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_eq, \u001b[96mself\u001b[39;49;00m.device.scalar_eq)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_eq'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_eq'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________ test_ewise_fn[cuda-shape0-greater_than] ____________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af28040>, shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:232: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mgreater_than\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a >= b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:470: in __ge__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_ge, \u001b[96mself\u001b[39;49;00m.device.scalar_ge)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_ge'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_ge'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape1-multiply] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0fd00>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:227: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mmultiply\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a * b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:442: in __mul__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_mul, \u001b[96mself\u001b[39;49;00m.device.scalar_mul\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_mul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_mul'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape1-divide] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0fd90>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:228: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mdivide\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a / b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:449: in __truediv__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_div, \u001b[96mself\u001b[39;49;00m.device.scalar_div\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_div'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_div'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape1-subtract] ______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0feb0>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:230: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33msubtract\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a - b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:435: in __sub__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m + (-other)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:453: in __neg__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m * (-\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:442: in __mul__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_mul, \u001b[96mself\u001b[39;49;00m.device.scalar_mul\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_mul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_mul'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_______________________ test_ewise_fn[cuda-shape1-equal] _______________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af0ff40>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:231: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mequal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a == b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:467: in __eq__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_eq, \u001b[96mself\u001b[39;49;00m.device.scalar_eq)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_eq'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_eq'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________ test_ewise_fn[cuda-shape1-greater_than] ____________________\u001b[0m\n",
      "\n",
      "fn = <function <lambda> at 0x75111af28040>, shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, OP_FNS, ids=OP_NAMES)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:248: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:232: in <lambda>\n",
      "    \u001b[0m\u001b[33m\"\u001b[39;49;00m\u001b[33mgreater_than\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[94mlambda\u001b[39;49;00m a, b: a >= b,\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:470: in __ge__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_ge, \u001b[96mself\u001b[39;49;00m.device.scalar_ge)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_ge'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_ge'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_ewise_max[cuda-shape0] __________________________\u001b[0m\n",
      "\n",
      "shape = (1, 1, 1), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_max\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           np.maximum(_A, _B), A.maximum(B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:259: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:462: in maximum\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_maximum, \u001b[96mself\u001b[39;49;00m.device.scalar_maximum\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_maximum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_maximum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_________________________ test_ewise_max[cuda-shape1] __________________________\u001b[0m\n",
      "\n",
      "shape = (4, 5, 6), device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ewise_shapes)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_max\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(*shape)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           np.maximum(_A, _B), A.maximum(B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:259: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:462: in maximum\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_maximum, \u001b[96mself\u001b[39;49;00m.device.scalar_maximum\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_maximum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_maximum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_scalar_mul[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_mul\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(A * \u001b[94m5.0\u001b[39;49;00m, (B * \u001b[94m5.0\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:372: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:442: in __mul__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_mul, \u001b[96mself\u001b[39;49;00m.device.scalar_mul\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_mul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_mul'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_scalar_div[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_div\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(A / \u001b[94m5.0\u001b[39;49;00m, (B / \u001b[94m5.0\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:379: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:449: in __truediv__\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_div, \u001b[96mself\u001b[39;49;00m.device.scalar_div\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_div'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_div'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_scalar_power[cuda] ____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_power\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           np.power(A, \u001b[94m5.0\u001b[39;49;00m), (B**\u001b[94m5.0\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:387: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:457: in __pow__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.scalar_power(\u001b[96mself\u001b[39;49;00m.compact()._handle, other, out._handle)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'scalar_power'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'scalar_power'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_scalar_maximum[cuda] ___________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_maximum\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        C = (np.max(A) + \u001b[94m1.0\u001b[39;49;00m).item()\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           np.maximum(A, C), (B.maximum(C)).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:400: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:462: in maximum\n",
      "    \u001b[0mother, \u001b[96mself\u001b[39;49;00m.device.ewise_maximum, \u001b[96mself\u001b[39;49;00m.device.scalar_maximum\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_maximum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_maximum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________________ test_scalar_eq[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_eq\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        C = A[\u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m].item()\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(A == C, (B == C).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:413: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:467: in __eq__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_eq, \u001b[96mself\u001b[39;49;00m.device.scalar_eq)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_eq'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_eq'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________________ test_scalar_ge[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_scalar_ge\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        C = A[\u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m].item()\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(A >= C, (B >= C).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:421: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:470: in __ge__\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.ewise_or_scalar(other, \u001b[96mself\u001b[39;49;00m.device.ewise_ge, \u001b[96mself\u001b[39;49;00m.device.scalar_ge)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_ge'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_ge'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________________ test_ewise_log[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_log\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.abs(np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.log(A), (B.log()).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:428: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:488: in log\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.ewise_log(\u001b[96mself\u001b[39;49;00m.compact()._handle, out._handle)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_log'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_log'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m_____________________________ test_ewise_exp[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_exp\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.exp(A), (B.exp()).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:435: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:493: in exp\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.ewise_exp(\u001b[96mself\u001b[39;49;00m.compact()._handle, out._handle)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_exp'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_exp'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m____________________________ test_ewise_tanh[cuda] _____________________________\u001b[0m\n",
      "\n",
      "device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_ewise_tanh\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n",
      "        A = np.random.randn(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(A, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose(np.tanh(A), (B.tanh()).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:442: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:498: in tanh\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.ewise_tanh(\u001b[96mself\u001b[39;49;00m.compact()._handle, out._handle)\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'ewise_tanh'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'ewise_tanh'. Did you mean: 'ewise_add'?\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape0-multiply]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape0-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape0-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape0-equal]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape0-greater_than]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape1-multiply]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape1-divide]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape1-subtract]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape1-equal]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_fn[cuda-shape1-greater_than]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_max[cuda-shape0]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_max[cuda-shape1]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_mul[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_div[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_power[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_maximum[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_eq[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_scalar_ge[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_log[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_exp[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_ewise_tanh[cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31m================= \u001b[31m\u001b[1m21 failed\u001b[0m, \u001b[32m2 passed\u001b[0m, \u001b[33m113 deselected\u001b[0m\u001b[31m in 0.93s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"(ewise_fn or ewise_max or log or exp or tanh or (scalar and not setitem)) and cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "db80e3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 9 items / 8 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m___________________________ submit_ndarray_cuda_ops ____________________________\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_ndarray_cuda_ops>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "func_name = 'ndarray_cuda_ops'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mstart_submission\u001b[39;49;00m(func_name):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\" Begin a submisssion to the mugrade server \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        server_url, protocol = get_server_url_protocol(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        response = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                                 params = {\u001b[33m\"\u001b[39;49;00m\u001b[33muser_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                                           \u001b[33m\"\u001b[39;49;00m\u001b[33mfunc_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: func_name},\u001b[90m\u001b[39;49;00m\n",
      "                                 verify=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m response.status_code != \u001b[94m200\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresponse.text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m response.json()[\u001b[33m\"\u001b[39;49;00m\u001b[33msubmission_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       KeyError: 'submission_key'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:103: KeyError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1msubmit_ndarray_cuda_ops\u001b[0m - KeyError: 'submission_key'\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[31m in 1.18s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cuda_ops\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b7de79b"
   },
   "source": [
    "## Part 8: CUDA Backend - Reductions\n",
    "\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cuda.cu`:\n",
    "\n",
    "* `ReduceMax()`\n",
    "* `ReduceSum()`\n",
    "\n",
    "You can take a fairly simplistic approach here, and just use a separate CUDA thread for each individual reduction item: i.e., if there is a 100 x 20 array you are reducing over the second dimension, you could have 100 threads, each of which individually processed its own 20-dimensional array..  This is particularly inefficient for the `.max(axis=None)` calls, but we won't worry about this for the time being.  If you want a more industrial-grade implementation, you use a hierarchical mechanism that first aggregated across some smaller span, then had a secondary function that aggregated across _these_ reduced arrays, etc.  But this is not needed to pass the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "10239c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 128 deselected / 8 selected                              \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params0-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 12%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params1-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 25%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params2-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 37%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_sum[params3-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 50%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params0-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 62%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params1-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 75%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params2-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 87%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_reduce_max[params3-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_sum[params0-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 0, 'dims': (10,)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_sum\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:113: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:583: in sum\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_sum(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_sum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_sum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_sum[params1-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 0, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_sum\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:113: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:583: in sum\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_sum(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_sum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_sum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_sum[params2-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 1, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_sum\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:113: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:583: in sum\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_sum(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_sum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_sum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_sum[params3-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 2, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_sum\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.sum(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:113: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:583: in sum\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_sum(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_sum'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_sum'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_max[params0-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 0, 'dims': (10,)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_max\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:124: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:588: in max\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_max(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_max'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_max'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_max[params1-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 0, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_max\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:124: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:588: in max\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_max(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_max'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_max'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_max[params2-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 1, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_max\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:124: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:588: in max\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_max(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_max'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_max'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_reduce_max[params3-cuda] _________________________\u001b[0m\n",
      "\n",
      "params = {'axis': 2, 'dims': (4, 5, 6)}, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, reduce_params)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_reduce_max\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n",
      "        dims, axis = params[\u001b[33m\"\u001b[39;49;00m\u001b[33mdims\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], params[\u001b[33m\"\u001b[39;49;00m\u001b[33maxis\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(*dims)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        np.testing.assert_allclose(\u001b[90m\u001b[39;49;00m\n",
      ">           _A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m), A.max(axis=axis, keepdims=\u001b[94mTrue\u001b[39;49;00m).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:124: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:588: in max\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.reduce_max(view.compact()._handle, out._handle, view.shape[-\u001b[94m1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'reduce_max'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'reduce_max'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_sum[params0-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_sum[params1-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_sum[params2-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_sum[params3-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_max[params0-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_max[params1-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_max[params2-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_reduce_max[params3-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m8 failed\u001b[0m, \u001b[33m128 deselected\u001b[0m\u001b[31m in 0.61s\u001b[0m\u001b[31m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"reduce and cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0f75df69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 9 items / 8 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ submit_ndarray_cuda_reductions ________________________\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_ndarray_cuda_reductions>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "func_name = 'ndarray_cuda_reductions'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mstart_submission\u001b[39;49;00m(func_name):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\" Begin a submisssion to the mugrade server \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        server_url, protocol = get_server_url_protocol(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        response = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                                 params = {\u001b[33m\"\u001b[39;49;00m\u001b[33muser_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                                           \u001b[33m\"\u001b[39;49;00m\u001b[33mfunc_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: func_name},\u001b[90m\u001b[39;49;00m\n",
      "                                 verify=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m response.status_code != \u001b[94m200\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresponse.text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m response.json()[\u001b[33m\"\u001b[39;49;00m\u001b[33msubmission_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       KeyError: 'submission_key'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:103: KeyError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1msubmit_ndarray_cuda_reductions\u001b[0m - KeyError: 'submission_key'\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[31m in 0.93s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cuda_reductions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01027bb7"
   },
   "source": [
    "## Part 9: CUDA Backend - Matrix multiplication\n",
    "\n",
    "Implement the following functions in `ndarray_backend_cuda.cu`:\n",
    "\n",
    "* `Matmul()`\n",
    "\n",
    "Finally, as your final exercise, you'll implement matrix multiplication on the GPU.  Your implementation here can roughly follow the presentation in class.  While you can pass the tests using fairly naive code here (i.e., you could just have a separate thread for each (i,j) location in the matrix, doing the matrix multiplication efficiently (to make it actually faster than a CPU version) requires cooperative fetching and the block shared memory register tiling covered in class.  Try to implement using these methods, and see how much faster you can get your code than the C++ (or numpy) backends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "aa7b46d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0 -- /home/user002/miniconda3/envs/LLMs/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "collected 136 items / 126 deselected / 10 selected                             \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py::test_matmul[16-16-16-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 10%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[8-8-8-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 20%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[1-2-3-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 30%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[3-4-5-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 40%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[5-4-3-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m                [ 50%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[64-64-64-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 60%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[72-72-72-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 70%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[72-73-74-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 80%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[74-73-72-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 90%]\u001b[0m\n",
      "tests/hw3/test_ndarray.py::test_matmul[128-128-128-cuda] \u001b[31mFAILED\u001b[0m\u001b[31m          [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[16-16-16-cuda] __________________________\u001b[0m\n",
      "\n",
      "m = 16, n = 16, p = 16, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[8-8-8-cuda] ____________________________\u001b[0m\n",
      "\n",
      "m = 8, n = 8, p = 8, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[1-2-3-cuda] ____________________________\u001b[0m\n",
      "\n",
      "m = 1, n = 2, p = 3, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[3-4-5-cuda] ____________________________\u001b[0m\n",
      "\n",
      "m = 3, n = 4, p = 5, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m___________________________ test_matmul[5-4-3-cuda] ____________________________\u001b[0m\n",
      "\n",
      "m = 5, n = 4, p = 3, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[64-64-64-cuda] __________________________\u001b[0m\n",
      "\n",
      "m = 64, n = 64, p = 64, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[72-72-72-cuda] __________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 72, p = 72, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[72-73-74-cuda] __________________________\u001b[0m\n",
      "\n",
      "m = 72, n = 73, p = 74, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m__________________________ test_matmul[74-73-72-cuda] __________________________\u001b[0m\n",
      "\n",
      "m = 74, n = 73, p = 72, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[31m\u001b[1m________________________ test_matmul[128-128-128-cuda] _________________________\u001b[0m\n",
      "\n",
      "m = 128, n = 128, p = 128, device = cuda()\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, matmul_dims)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n",
      "        _A = np.random.randn(m, n)\u001b[90m\u001b[39;49;00m\n",
      "        _B = np.random.randn(n, p)\u001b[90m\u001b[39;49;00m\n",
      "        A = nd.array(_A, device=device)\u001b[90m\u001b[39;49;00m\n",
      "        B = nd.array(_B, device=device)\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_allclose((A @ B).numpy(), _A @ _B, rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/hw3/test_ndarray.py\u001b[0m:365: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:549: in __matmul__\n",
      "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device.matmul(\u001b[90m\u001b[39;49;00m\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "self = cuda(), name = 'matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, name):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m \u001b[96mgetattr\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.mod, name)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no attribute 'matmul'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:28: AttributeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[16-16-16-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[8-8-8-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[1-2-3-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[3-4-5-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[5-4-3-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[64-64-64-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[72-72-72-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[72-73-74-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[74-73-72-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1mtest_matmul[128-128-128-cuda]\u001b[0m - AttributeError: module 'needle.backend_ndarray.ndarray_backend_cuda' has no...\n",
      "\u001b[31m====================== \u001b[31m\u001b[1m10 failed\u001b[0m, \u001b[33m126 deselected\u001b[0m\u001b[31m in 0.65s\u001b[0m\u001b[31m ======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"matmul and cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3ae22cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.14, pytest-8.3.4, pluggy-1.5.0\n",
      "rootdir: /home/user002/code/AI_fram/CMU-10414/hw3\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mUsing needle backend\n",
      "collected 9 items / 8 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "tests/hw3/test_ndarray.py \u001b[31mF\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m__________________________ submit_ndarray_cuda_matmul __________________________\u001b[0m\n",
      "\n",
      "pyfuncitem = <Function submit_ndarray_cuda_matmul>\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.hookimpl(hookwrapper=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mpytest_pyfunc_call\u001b[39;49;00m(pyfuncitem):\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m## prior to test, initialize submission\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mglobal\u001b[39;49;00m _values, _submission_key, _errors\u001b[90m\u001b[39;49;00m\n",
      "        _values = []\u001b[90m\u001b[39;49;00m\n",
      "        _errors = \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        func_name = pyfuncitem.name[\u001b[94m7\u001b[39;49;00m:]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_OP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == \u001b[33m\"\u001b[39;49;00m\u001b[33msubmit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      ">           _submission_key = start_submission(func_name)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:164: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "func_name = 'ndarray_cuda_matmul'\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mstart_submission\u001b[39;49;00m(func_name):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\" Begin a submisssion to the mugrade server \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        server_url, protocol = get_server_url_protocol(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
      "        response = requests.post(server_url + \u001b[33m\"\u001b[39;49;00m\u001b[33msubmission\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
      "                                 params = {\u001b[33m\"\u001b[39;49;00m\u001b[33muser_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mMUGRADE_KEY\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],\u001b[90m\u001b[39;49;00m\n",
      "                                           \u001b[33m\"\u001b[39;49;00m\u001b[33mfunc_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: func_name},\u001b[90m\u001b[39;49;00m\n",
      "                                 verify=\u001b[94mFalse\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m response.status_code != \u001b[94m200\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mException\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mError : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mresponse.text\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94mreturn\u001b[39;49;00m response.json()[\u001b[33m\"\u001b[39;49;00m\u001b[33msubmission_key\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       KeyError: 'submission_key'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m../../../../miniconda3/envs/LLMs/lib/python3.10/site-packages/mugrade/mugrade.py\u001b[0m:103: KeyError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/hw3/test_ndarray.py::\u001b[1msubmit_ndarray_cuda_matmul\u001b[0m - KeyError: 'submission_key'\n",
      "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[31m in 0.90s\u001b[0m\u001b[31m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m mugrade submit \"YOUR KEY HERE\" -k \"ndarray_cuda_matmul\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
